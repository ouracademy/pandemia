# -*- coding: utf-8 -*-
"""HIBRIDO7a2_2_V6ArabiaSaudita_Reg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13EIOwUc2Pf68Kz4GqpWn__6xlnzjfbYI

ALGORITMO HIBRIDO 7: RF, SVM, GBM, DT, XGBOOST, LinearReg Y LSTM. Ensemble con GBM por separado. Incluye explicabilidad LIME de cada algoritmo en el resultado Final y También incluye Shap
"""

!pip install https://bit.ly/3o4smsZ

pip install lime

pip install scikit-learn pandas numpy

pip install tensorflow

pip install keras

!pip install keras-tuner -q

!pip install shap

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from xgboost import XGBRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from keras_tuner.tuners import RandomSearch
from sklearn.impute import SimpleImputer
import lime
from lime.lime_tabular import LimeTabularExplainer
import shap

from google.colab import drive
drive.mount('/content/drive')

df1_positivos = pd.read_csv("/content/drive/MyDrive/Desarrollo/DataPorPais/daily_cases_ksa_covid19ArabiaSauditaDepuradoReducedPCA.csv",sep=",")

df1_positivos.info()
df1_positivos

df = pd.DataFrame(df1_positivos)
maximos = df.max()
minimos = df.min()
promedios = df.mean()

# Mostrar los resultados
resultados = pd.DataFrame({
    'Máximo': maximos,
    'Mínimo': minimos,
    'Promedio': promedios
})
print(resultados)

# Obtener los nombres de las columnas
nombres_columnas = df1_positivos.columns.tolist()
# Mostrar los nombres de las columnas
print("Nombres de las columnas:")
print(nombres_columnas)

#df1_positivos= df1_positivos.drop('Unnamed: 0', axis=1)  # se ejecuta una sola vez

df1_positivos.rename(columns={'PC1_daily_collected_sample_total_deaths': 'Confirmed_entero'}, inplace=True)  #  Bangladesh
#df1_positivos['Confirmed_entero'] = df1_positivos['total_confirmed'].astype(int)      #  Bangladesh

df1_positivos.info()
df1_positivos

df1_positivos.rename(columns={'PC1_Confirmed_Tested': 'Confirmed_entero'}, inplace=True)
df1_positivos['Confirmed_entero'] = df1_positivos['Confirmed_entero'].astype(int)

data=df1_positivos

# Cargar un conjunto de datos de ejemplo
# Dividir el conjunto de datos en características (X) y etiquetas (y)
X = data.drop('Confirmed_entero', axis=1)
y = data['Confirmed_entero']

# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Definir los parámetros de búsqueda para cada modelo
param_grids = {
    "RF": {"n_estimators": [50, 100, 200], "max_depth": [5, 10, 15]},
    "SVM": {"C": [0.1, 1.0, 10], "epsilon": [0.01, 0.1, 0.2]},
    "GBM": {"n_estimators": [50, 100, 200], "learning_rate": [0.01, 0.1, 0.2], "max_depth": [3, 5, 7]},
    "DT": {"max_depth": [5, 10, 15]},
    "XGB": {"n_estimators": [50, 100, 200], "learning_rate": [0.01, 0.1, 0.2], "max_depth": [3, 5, 7]},
}

# Diccionario para almacenar las mejores instancias de modelos
best_models = {}

# Buscar mejores hiperparámetros usando GridSearchCV
for model_name, param_grid in param_grids.items():
    print(f"Buscando mejores hiperparámetros para {model_name}...")
    if model_name == "RF":
        model = RandomForestRegressor(random_state=42)
    elif model_name == "SVM":
        model = SVR()
    elif model_name == "GBM":
        model = GradientBoostingRegressor(random_state=42)
    elif model_name == "DT":
        model = DecisionTreeRegressor(random_state=42)
    elif model_name == "XGB":
        model = XGBRegressor(random_state=42, use_label_encoder=False, eval_metric="rmse")

    grid_search = GridSearchCV(model, param_grid, scoring="neg_mean_squared_error", cv=5, n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_models[model_name] = grid_search.best_estimator_
    print(f"Mejores parámetros para {model_name}: {grid_search.best_params_}")
# Agregar regresión lineal sin necesidad de búsqueda de hiperparámetros
best_models["Linear Reg."] = LinearRegression()

# Entrenamiento de modelos y predicciones
predictions = {}
metrics = {}

# Función para calcular las métricas
def calculate_metrics(y_true, y_pred):
    return {
        "MAE": mean_absolute_error(y_true, y_pred),
        "MSE": mean_squared_error(y_true, y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "R2": r2_score(y_true, y_pred)
    }

# Predicciones con modelos tradicionales
for model_name, model in best_models.items():
    if model_name == 'LSTM':
        continue
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    predictions[model_name] = y_pred
    metrics[model_name] = calculate_metrics(y_test, y_pred)

# Reorganizar los datos en un formato compatible con LSTM
X_train_lstm = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))
X_test_lstm = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))

# Definir la función de creación del modelo LSTM para el ajuste de hiperparámetros
def build_lstm_model(hp):
    model = Sequential()
    # Número de unidades LSTM
    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32),
                   activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))
    model.add(Dense(1))
    # Tasa de aprendizaje del optimizador Adam
    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')
    return model

# Configurar KerasTuner para la búsqueda de hiperparámetros
tuner = RandomSearch(
    build_lstm_model,
    objective='val_loss',
    max_trials=10,  # Número de combinaciones de hiperparámetros a probar
    executions_per_trial=2,  # Número de ejecuciones para cada conjunto de hiperparámetros
    directory='lstm_tuning',
    project_name='LSTM_hyperparameter_tuning'
)

# Realizar la búsqueda de ls mejores hiperparámetros para LSTM
tuner.search(X_train_lstm, y_train, epochs=50, validation_split=0.2, batch_size=32)
# Obtener el mejor modelo
best_lstm_model = tuner.get_best_models(num_models=1)[0]
# Entrenar el mejor modelo con el conjunto de prueba
best_lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=0)
y_pred_lstm = best_lstm_model.predict(X_test_lstm)
# Asegurarse de que la salida de LSTM sea un arreglo unidimensional
predictions["LSTM"] = y_pred_lstm #.flatten()
best_models["LSTM"] = best_lstm_model

# Función para calcular las métricas LSTM
def calculate_metricsLSTM(y_true, y_pred):
    return {
        "MAE": mean_absolute_error(y_true, y_pred),
        "MSE": mean_squared_error(y_true, y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "R2": r2_score(y_true, y_pred)
    }

# Calcular las métricas de rendimiento
metrics = calculate_metricsLSTM(y_test, y_pred_lstm.flatten())
#metrics[model_name] = calculate_metrics(y_test, y_pred)
#metrics['LSTM'] = calculate_metrics(y_test, y_pred)
print("Mejores hiperparámetros encontrados:", tuner.get_best_hyperparameters()[0].values)
print("Métricas del modelo LSTM ajustado:")
print(f"MAE: {metrics['MAE']}")
print(f"MSE: {metrics['MSE']}")
print(f"RMSE: {metrics['RMSE']}")
print(f"R2 Score: {metrics['R2']}")

#lstm_model.fit(X_train_reshaped, y_train_reshaped, epochs=..., batch_size=...)  # Ajuste de los parámetros según sea necesario
#y_pred_lstm = lstm_model.predict(X_test_reshaped)
predictions['LSTM'] = y_pred_lstm

# Predicciones con cada modelo
metrics = {}  # Inicializar métricas como un diccionario vacío
for model_name, model in best_models.items():
    if model_name == "LSTM":  # Si es LSTM
        y_pred_lstm = model.predict(X_test_lstm) # Obtener predicciones para LSTM
        # Asignar métricas como un diccionario anidado para LSTM
        metrics[model_name] = calculate_metricsLSTM(y_test, y_pred_lstm.flatten())
    else:  # para otros modelos
        y_pred = model.predict(X_test)  # Suponiendo que X_test es la entrada para otros modelos
        # Asignar métricas como un diccionario anidado para otros modelos
        metrics[model_name] = calculate_metrics(y_test, y_pred)

# Escribir las métricas para cada modelo
for model_name, model_metrics in metrics.items():
    print(f"Metrics for {model_name}:")
    for metric_name, value in model_metrics.items(): # Iterar a través del diccionario de métricas
        print(f"  {metric_name}: {value}")
    print()

for model_name in predictions:
    predictions[model_name] = predictions[model_name].flatten()
# Crear DataFrame de predicciones como entrada para el modelo GBM final
stacked_predictions = pd.DataFrame(predictions)
# Entrenar modelo GBM final con las predicciones de los modelos base
mm_gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)
mm_gbm.fit(stacked_predictions, y_test)

# Predicción final y métricas
y_pred_final = mm_gbm.predict(stacked_predictions)
# Almacenar métricas para el modelo final en un diccionario
metrics["Final_GBM"] = calculate_metrics(y_test, y_pred_final)

# Mostrar las métricas
print("Métricas del meta modelo mm_GBM final")
# Acceder y mostrar las métricas del modelo 'Final_GBM' directamente
final_gbm_metrics = metrics["Final_GBM"]
print(f"  MAE: {final_gbm_metrics['MAE']}")
print(f"  MSE: {final_gbm_metrics['MSE']}")
print(f"  RMSE: {final_gbm_metrics['RMSE']}")
print(f"  R2 Score: {final_gbm_metrics['R2']}\n")

pred_df = stacked_predictions.copy()

# LIME para explicabilidad
# Crear un explicador LIME para el meta-modelo
# se usan los nombres de pred_df
explainer_meta = LimeTabularExplainer(pred_df.values,
                                     mode='regression',
                                     feature_names=pred_df.columns,
                                     discretize_continuous=True)

# Explicar un ejemplo del conjunto de test para el meta-modelo

i = 20  # Elige un índice para explicar
exp_meta = explainer_meta.explain_instance(pred_df.iloc[i].values, mm_gbm.predict)

# Mostrar explicaciones
print(f'Explicación LIME para el meta-modelo SVM en el ejemplo {i}:')
exp_meta.show_in_notebook()
# Añadir explicación en palabras
def explain_lime_in_words(exp):
    explanation = exp.as_list()  # Obtener la explicación como lista [(característica, peso)]
    positive_factors = []
    negative_factors = []

    # Clasificar las características en positivas y negativas
    for feature, weight in explanation:
        if weight > 0:
            positive_factors.append(f"{feature} aumenta la predicción con un peso de {round(weight, 4)}")
        else:
            negative_factors.append(f"{feature} disminuye la predicción con un peso de {round(weight, 4)}")

    # Generar la explicación en palabras
    explanation_text = "Explicación en palabras:\n"

    if positive_factors:
        explanation_text += "Las siguientes características tienen un efecto positivo en la predicción del meta-modelo:\n"
        for factor in positive_factors:
            explanation_text += f"- {factor}\n"

    if negative_factors:
        explanation_text += "\nLas siguientes características tienen un efecto negativo en la predicción del meta-modelo:\n"
        for factor in negative_factors:
            explanation_text += f"- {factor}\n"

    return explanation_text

# Generar y mostrar la explicación en palabras
explanation_in_words = explain_lime_in_words(exp_meta)
print(explanation_in_words)

# Crea una función que envuelva la predicción del modelo
def model_wrapper(X):
    # Convertir la entrada en DataFrame si es necesario
    if not isinstance(X, pd.DataFrame):
        X = pd.DataFrame(X, columns=pred_df.columns)
    return mm_gbm.predict(X)

# Crear el explicador SHAP utilizando la función wrapper
explainer_shap = shap.KernelExplainer(model_wrapper, pred_df)

# Crear el explicador SHAP utilizando la función wrapper
explainer_shap = shap.KernelExplainer(model_wrapper, pred_df)

# Calcular valores SHAP para un subconjunto de datos o para todo el conjunto de datos
# Aquí, utilizamos las primeras 100 instancias como ejemplo.
shap_values = explainer_shap.shap_values(pred_df.iloc[:100])  # Cambiado a múltiples instancias

# Mostrar explicaciones SHAP (gráfico de barras)
# Tener en cuenta que ahora estamos usando los valores shap_values ​​modificados.
shap.summary_plot(shap_values, pred_df.iloc[:100], feature_names=pred_df.columns)

# Muestra una explicación detallada de un ejemplo específico (aún usando i)
shap.force_plot(explainer_shap.expected_value, shap_values[i], pred_df.iloc[i], feature_names=pred_df.columns)
# Función para explicar valores SHAP en palabras para una sola instancia
def explain_shap_values_in_words(shap_values, features, instance_index):
    explanation = ""
    instance_shap_values = shap_values[instance_index]
    feature_names = features.columns

    # Ordenar características por valor SHAP absoluto
    sorted_indices = sorted(range(len(instance_shap_values)), key=lambda i: abs(instance_shap_values[i]), reverse=True)

    for feature_index in sorted_indices:
        shap_value = instance_shap_values[feature_index]
        feature_name = feature_names[feature_index]

        if shap_value > 0:
            explanation += f"- {feature_name} aumentó la predicción en {shap_value:.2f}.\n"
        else:
            explanation += f"- {feature_name} disminuyó la predicción en {-shap_value:.2f}.\n"

    return explanation

# Elegir una instancia para explicar
instance_index = 5

# Generar y escribir la explicación en palabras.
explanation_in_words = explain_shap_values_in_words(shap_values, pred_df, instance_index)
print(f"\nExplicación por instancia {instance_index}:\n{explanation_in_words}") # Se cambió values_in_words a explanation_in_words

#Mostrar una explicación detallada de un ejemplo específico
shap.force_plot(explainer_shap.expected_value, shap_values[instance_index], pred_df.iloc[instance_index], feature_names=pred_df.columns,matplotlib=True)

#Predicción de los siguientes 14 valores
N = 30

pred_df

last_N_data = df1_positivos.tail(N)

X_last_N = last_N_data.drop('Confirmed_entero', axis=1)  # si es necesario se modifica

X_last_N_lstm = np.reshape(X_last_N.values, (X_last_N.shape[0], 1, X_last_N.shape[1]))

# Predicciones por día individuales
individual_predictions = {}
for name, model in best_models.items():          # models
    if name == 'LSTM':  # Check if the model is LSTM
        individual_predictions[name] = model.predict(X_last_N_lstm) # Use reshaped data for LSTM
    else:
        individual_predictions[name] = model.predict(X_last_N)

# predicciones del Meta-modelo
meta_predictions = mm_gbm.predict(pred_df)
#Almacenar y mostrar predicciones: se crea una lista o un DataFrame para almacenar los valores predichos.

# Lista de predicciones con etiquetas para los próximos N días
next_N_predictions = list(meta_predictions)  # Reemplazar con las predicciones requeridas
labels = ['Predicción día ' + str(i + 1) + ': ' for i in range(len(next_N_predictions))]  # Crear etiquetas

print("Predicciones para los próximos  " + str(N) + " valores:")
for label, prediction in zip(labels, next_N_predictions):
    print(label, prediction)  # Imprimir etiqueta y predicción juntas

# Si es que 'next_N_predictions' es una lista de predicciones
predictions_df = pd.DataFrame({'Predicción_Confirmado': next_N_predictions})
# Crear etiquetas para el índice
labels = ['Predicción día ' + str(i + 1) for i in range(len(next_N_predictions))]
predictions_df.index = labels  # Asignar etiquetas al índice

print(predictions_df)

actual_cases=data['Confirmed_entero']
predicted_cases=predictions_df
# Generar el gráfico
# Generar una secuencia de fechas en lugar de un único valor
dates = np.arange(1, len(actual_cases) + 1)  # Crea una matriz desde 1 hasta el número de casos reales

# Crea el gráfico
plt.figure(figsize=(12, 6))
plt.plot(dates, actual_cases, label='Casos Actuales', marker='o')
plt.plot(dates[:len(predicted_cases)], predicted_cases, label='Casos Predichos', marker='x') # Graficar los casos previstos frente a las fechas correspondientes

# Personaliza el gráfico
plt.title('Predicciones de casos de COVID-19 vs. casos reales')
plt.xlabel('Día')
plt.ylabel('Número de casos')
plt.legend()
plt.grid(True)

# Girar las etiquetas del eje x para una mejor legibilidad (opcional)
plt.xticks(rotation=45, ha='right')

# Mostrar el gráfico
plt.tight_layout()
plt.show()

